---
title: Fine-tuning Techniques
---

# Fine-tuning Large Language Models

Fine-tuning adapts a pre-trained language model to a specific task or domain. As models have grown larger, full fine-tuning — updating all parameters — has become increasingly expensive, driving the development of parameter-efficient methods.

## Full Fine-tuning

In full fine-tuning, all model parameters are updated using task-specific training data. This approach generally achieves the best performance but requires storing a separate copy of all parameters for each task. For a 7-billion-parameter model, this means roughly 28 GB of storage per task in float32, or 14 GB in float16.

The learning rate for fine-tuning is typically much lower than for pre-training, usually in the range of 1e-5 to 5e-5. Training for too many epochs can lead to catastrophic forgetting, where the model loses its general capabilities. Most fine-tuning runs use 1-3 epochs.

## LoRA: Low-Rank Adaptation

LoRA (Low-Rank Adaptation) freezes the pre-trained weights and injects trainable low-rank matrices into each Transformer layer. Instead of updating a weight matrix W directly, LoRA adds a low-rank decomposition BA where B and A have much smaller dimensions. Typically, the rank r is set to 8 or 16, reducing trainable parameters by 10,000x compared to full fine-tuning.

LoRA is applied to attention projection matrices (query, key, value, and output projections). The alpha parameter controls the scaling of the low-rank update. A common configuration uses rank 8, alpha 16, and applies LoRA to all linear layers.

QLoRA extends LoRA by quantizing the frozen base model to 4-bit precision using NormalFloat4 quantization and paged optimizers. This allows fine-tuning a 65-billion-parameter model on a single 48 GB GPU. The double quantization technique further reduces memory by quantizing the quantization constants.

## Prefix Tuning and Prompt Tuning

Prefix tuning prepends trainable continuous vectors (prefixes) to the key and value sequences at each Transformer layer. Only the prefix parameters are trained while the model remains frozen. This typically adds 0.1% to 1% additional parameters.

Prompt tuning is a simpler variant that only adds trainable tokens to the input embedding layer. Despite its simplicity, prompt tuning approaches full fine-tuning performance as model size increases. For models above 10 billion parameters, prompt tuning matches full fine-tuning on many benchmarks.

## Instruction Tuning

Instruction tuning fine-tunes a model on a diverse collection of tasks described via natural language instructions. Datasets like FLAN, Natural Instructions, and the Open Assistant Conversations dataset provide thousands of task descriptions with examples.

The key insight of instruction tuning is that training on a broad set of instructed tasks improves zero-shot performance on unseen tasks. FLAN-T5 and FLAN-PaLM demonstrated substantial improvements over their base models across many benchmarks without task-specific fine-tuning.

## Direct Preference Optimization (DPO)

DPO is an alternative to RLHF that directly optimizes the language model on preference data without training a separate reward model. The DPO loss function implicitly defines a reward using the ratio of log-probabilities between the policy and a reference model.

DPO is simpler to implement than RLHF, more stable during training, and achieves comparable or better performance. It requires pairs of preferred and dispreferred responses for each prompt, which can be collected from human annotators or generated by the model itself.
